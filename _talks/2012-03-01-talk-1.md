---
title: "Real time Geometry Extraction and Vectorization of Objects from Point-Cloud Data"
collection: talks
type: ""
permalink: /talks/2012-03-01-talk-1
venue: "Imaging, Vision & Pattern Recognition Group (IVPR)"
date: 2020-12-12
location: "Jadavpur University, Kolkata, India"
---
Tag: 3D Vision, Machine Vision, Robot Vision, LiDAR, Point Cloud

  
Venue: [Imaging, Vision & Pattern Recognition Group (IVPR)](https://sites.google.com/site/ivprgroup/home-page-ivpr?authuser=0) <br/>
Collaboration: [Department of Electronics and Telecommunication Engineering](http://www.jaduniv.edu.in/view_department.php?deptid=84) & [Department of Computer Science and Engineering](http://www.jaduniv.edu.in/view_department.php?deptid=59), [Jadavpur University](http://www.jaduniv.edu.in/), Kolkata, India <br/>
Supervisor : Prof. [Ananda Shankar Chowdhury](https://sites.google.com/site/anandachowdhury/), Prof. [Sanjoy Kumar Saha](https://scholar.google.co.in/citations?user=MVooqJUAAAAJ&hl=en) <br/>
Timeline: July’ 17 - Dec’ 20 <br/>
 * Establishment of a pipeline for real time reconstruction of geometric objects from point cloud data. Fast surface segmentation, semantic labeling, and object grouping are the major constituting subjects of the pipeline

Single snapshot rotating LiDAR scan  
  ![alt text](https://github.com/jasorsi13/jasorsi.github.io/blob/master/paper_img/1.PNG?raw=true)" <br/>
  (a) Synthetic scene with scanned point cloud overlayed (b) Point cloud with distance color coded from blue(least) to red(highest)
  
  ![alt text](https://github.com/jasorsi13/jasorsi.github.io/blob/master/paper_img/2a.PNG?raw=true)" <br/>
  Block diagram of the entire system, the first stage can be merged with Lidar scanning by improvising the Lidar firmware

  ![alt text](https://github.com/jasorsi13/jasorsi.github.io/blob/master/paper_img/3.PNG?raw=true)" <br/>
  (a) A schematic showing the formation of point cloud by Lidar and (b) the resultant point cloud
 
  The proposed methodology segments the surface from a point cloud obtained by spinning Lidars only. Spinning Lidars work on the principle of spinning a vertical array   of divergent laser distance sensors and thus extracts point clouds in spherical coordinates. The point cloud consists of a set of coordinates P = {p(θ, φ, r)} where   θ is the fixed vertical angle of a sensor from the plane perpendicular to the spinning axis, φ is the variable horizontal angle due to spinning of the array and r     is the distance measured by the laser sensor. This form of representation is exploited by our methodology to structure the data in an ordered form, the only caveat     being running it for a single spin. By varying the factor of sub-sampling of φ, the horizontal density of the point cloud can be varied. Above figure shows the         operational procedure of a spinning Lidar and the resultant point cloud for an object with multiple surfaces. Please note that not every point during the sweep is     considered for mesh construction as noisy points too close to each other horizontally produce erroneous normal. Sub-sampling is done to rectify this error by           skipping points uniformly during the spin.

  ![alt text](https://github.com/jasorsi13/jasorsi.github.io/blob/master/paper_img/4.PNG?raw=true)" 
  

  ![alt text](https://github.com/jasorsi13/jasorsi.github.io/blob/master/paper_img/5.PNG?raw=true)" <br/>
Segmentation by Surface Homogeneity: Based on the normal at a point, as computed in the previous step, we now propagate the surface label. A label map L = {< p, l >|   p ∈ P, l = 0} is used for this purpose. This label map stores the label of each point p by assigning a label l. If for any point p, its l = 0 denotes the point is yet to be labeled. The criteria of assigning the label of p to its neighbor q depends on the absolute difference of their normal components. Three thresholds I, J, K   are empirically set depending on the type of environment. Segment labeling is propagated by a depth first search approach as described in algorithm 1. Two neighboring points will have the same label provided the absolute difference of corresponding components of their normals are within component-wise threshold. Computations of normals and mesh, as discussed earlier, generate the normal map N and the mesh M respectively. Subsequently algorithm 1 uses N and M to label the whole sub-sampled point cloud in an inductive fashion. Due to sub-sampling, all points in P will not get a label. This issue is resolved by assigning the label of its nearest labeled point along the horizontal sweep. An optional post-processing may be arranged by eliminating segments with too few points. 

  ![alt text](https://github.com/jasorsi13/jasorsi.github.io/blob/master/paper_img/6.PNG?raw=true)" <br/>
  (a) Synthetic scene with scanned point cloud overlayed (b) Point cloud with distance color coded from blue(least) to red(highest) (c) Mesh and normals with             subsampling factor of 5 (d) Point cloud segment surface ground truth (e) A detailed look at the mesh and normals (f) Segmented point cloud by proposed methodology
 
 





 
