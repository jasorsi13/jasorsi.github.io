---
title: "Real time Geometry Extraction and Vectorization of Objects from Point-Cloud Data"
collection: talks
type: ""
permalink: /talks/2012-03-01-talk-1
venue: "Imaging, Vision & Pattern Recognition Group (IVPR)"
date: 2020-12-12
location: "Jadavpur University, Kolkata, India"
---
Tag: 3D Vision, Machine Vision, Robot Vision, LiDAR, Point Cloud

  
Venue: [Imaging, Vision & Pattern Recognition Group (IVPR)](https://sites.google.com/site/ivprgroup/home-page-ivpr?authuser=0) <br/>
Collaboration: [Department of Electronics and Telecommunication Engineering](http://www.jaduniv.edu.in/view_department.php?deptid=84) & [Department of Computer Science and Engineering](http://www.jaduniv.edu.in/view_department.php?deptid=59), [Jadavpur University](http://www.jaduniv.edu.in/), Kolkata, India <br/>
Supervisor : Prof. [Ananda Shankar Chowdhury](https://sites.google.com/site/anandachowdhury/), Prof. [Sanjoy Kumar Saha](https://scholar.google.co.in/citations?user=MVooqJUAAAAJ&hl=en) <br/>
Timeline: July’ 17 - Dec’ 20 <br/>
 * Establishment of a pipeline for real time reconstruction of geometric objects from point cloud data. Fast surface segmentation, semantic labeling, and object grouping are the major constituting subjects of the pipeline

Single snapshot rotating LiDAR scan  
  ![alt text](https://github.com/jasorsi13/jasorsi.github.io/blob/master/paper_img/1.PNG?raw=true)" <br/>
  (a) Synthetic scene with scanned point cloud overlayed (b) Point cloud with distance color coded from blue(least) to red(highest)
  
  ![alt text](https://github.com/jasorsi13/jasorsi.github.io/blob/master/paper_img/2a.PNG?raw=true)" <br/>
  Block diagram of the entire system, the first stage can be merged with Lidar scanning by improvising the Lidar firmware

  ![alt text](https://github.com/jasorsi13/jasorsi.github.io/blob/master/paper_img/3.PNG?raw=true)" <br/>
  (a) A schematic showing the formation of point cloud by Lidar and (b) the resultant point cloud
 
  The proposed methodology segments the surface from a point cloud obtained by spinning Lidars only. Spinning Lidars work on the principle of spinning a vertical array   of divergent laser distance sensors and thus extracts point clouds in spherical coordinates. The point cloud consists of a set of coordinates P = {p(θ, φ, r)} where   θ is the fixed vertical angle of a sensor from the plane perpendicular to the spinning axis, φ is the variable horizontal angle due to spinning of the array and r     is the distance measured by the laser sensor. This form of representation is exploited by our methodology to structure the data in an ordered form, the only caveat     being running it for a single spin. By varying the factor of sub-sampling of φ, the horizontal density of the point cloud can be varied. Above figure shows the         operational procedure of a spinning Lidar and the resultant point cloud for an object with multiple surfaces. Please note that not every point during the sweep is     considered for mesh construction as noisy points too close to each other horizontally produce erroneous normal. Sub-sampling is done to rectify this error by           skipping points uniformly during the spin.

  ![alt text](https://github.com/jasorsi13/jasorsi.github.io/blob/master/paper_img/4.PNG?raw=true)" 
  
  Mesh Construction: The significant novelty of this work is the fast mesh generation process that enables quick realization of subsequent steps. The mesh construction   is a simultaneous process during the data acquisition stage. The connections are done during sampling in the following manner. Let a point be denoted as p(θ, φ, r).   Let the range of θ be [θ0, θn] which corresponds to n + 1 vertical sensors in the array; and the range of φ be [φ0, φm] where m + 1 is the number of times the         sensor array is sampled uniformly during a single spin. The corresponding distance of the point is r from the Lidar sensor. Let the topmost sensor in the array         corresponds to angle θ0 and the count proceeds from top to bottom the vertically spinning array. Further, let first shot during the spin corresponds to φ0. The mesh   is constructed by joining neighbouring points in the following manner: p(θi , φj , r) is joined with p(θi+1, φj , r), p(θi+1, φj+1, r) and p(θi , φj+1, r) for all     points within range of [θ0, θn−1] and [φ0, φm−1]. The points from the last vertical sensor, i.e., corresponding to θn, are joined with the immediate horizontal         neighbour. Thus, p(θn, φj , r) is joined with p(θn, φj+1, r) where j varies from 0 to m − 1. For points corresponding to φm, p(θi , φm, r) is joined with p(θi , φ0,   r), p(θi+1, φ0, r) and p(θi+1, φm, r). The point p(θn, φm, r) is joined with p(θn, φ0, r) to create the whole cylindrical connected mesh. For all pairs the joining     is done if both the points have an r that is within range of the Lidar. If all the neighbours of a point is present then six of them are connected by the meshing       technique instead of all eight. This is done to ensure there are no overlapping surface triangles. Figure 3(a) shows the connectivity a point which have six valid     neighbours on the mesh. The mesh is stored in a map of vectors M = {< p, v >| p ∈ P, v = {qn | qn ∈ P, n ≤ 6, and qn is a neighbor of p}} where each point is mapped   to the vector v containing its existent neighbors in an ordered fashion. The computational complexity of the meshing stage in O(nsp) where nsp is the number of sub-   sampled points. As the meshing is performed on the fly with the sensor spinning the absolute time depends on the angular frequency and sub-sampling factor.

  Normal Estimation: The structured mesh created in the previous step helps toward a fast computation of normal at a point. A point forms vectors with its neighbor.     Pair of vectors are taken in an ordered fashion. Normal is estimated for that point by averaging the resultant vectors formed by cross multiplication of those pairs.   The ordering is performed during the mesh construction stage only. For a point p(θi , φj , r), vectors are formed with the existing neighbors as stored in M in an     anti-clockwise fashion. A normal can be estimated for p if its corresponding v has |v| ≥ 2. From the neighbor vector v of p obtained from M, let p(θi , φj , r) forms   A by joining with p(θi+1, φj , r), B by joining with p(θi+1, φj+1, r), and so on until it forms F by joining with p(θi , φj−1, r). Then cross- multiplication of       existing consecutive vectors is performed. In general, if every point exists, then A × B, B × C etc. are computed ending with F × A to complete the circle. This       arrangement is illustrated in Figure 3(b). The normal is estimated by averaging all the ˆi, ˆj, ˆk components of the resultant vectors individually. The normals are   stored in the map N = {< p, n >| p ∈ P, n = {iˆp, jˆp, ˆkp}}. Due to the inherent nature of the meshing technique, sometime points from disconnected objects get       connected to the mesh. To mitigate this effect of a different surface contributing to the normal estimation, weighted average is used. The weight of a vector formed   by cross multiplication of an ordered pair is inversely proportional to the sum of lengths of the vectors in the pair. 

  Segmentation by Surface Homogeneity: Based on the normal at a point, as computed in the previous step, we now propagate the surface label. A label map L = {< p, l >|   p ∈ P, l = 0} is used for this purpose. This label map stores the label of each point p by assigning a label l. If for any point p, its l = 0 denotes the point is     yet to be labeled. The criteria of assigning the label of p to its neighbor q depends on the absolute difference of their normal components. Three thresholds I, J, K   are empirically set depending on the type of environment. Segment labeling is propagated by a depth first search approach as described in algorithm 1. Two             neighboring points will have the same label provided the absolute difference of corresponding components of their normals are within component-wise threshold.         Computations of normals and mesh, as discussed earlier, generate the normal map N and the mesh M respectively. Subsequently algorithm 1 uses N and M to label the       whole sub-sampled point cloud in an inductive fashion. Due to sub-sampling, all points in P will not get a label. This issue is resolved by assigning the label of     its nearest labeled point along the horizontal sweep. An optional post-processing may be arranged by eliminating segments with too few points. 
 
  ![alt text](https://github.com/jasorsi13/jasorsi.github.io/blob/master/paper_img/5.PNG?raw=true)" <br/>
  ![alt text](https://github.com/jasorsi13/jasorsi.github.io/blob/master/paper_img/6.PNG?raw=true)" <br/>
  (a) Synthetic scene with scanned point cloud overlayed (b) Point cloud with distance color coded from blue(least) to red(highest) (c) Mesh and normals with             subsampling factor of 5 (d) Point cloud segment surface ground truth (e) A detailed look at the mesh and normals (f) Segmented point cloud by proposed methodology
 
 





 
